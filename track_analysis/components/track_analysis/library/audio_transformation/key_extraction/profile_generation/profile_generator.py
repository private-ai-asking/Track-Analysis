# import pprint
# from concurrent.futures import ThreadPoolExecutor, as_completed
# from pathlib import Path
# from typing import List
#
# import numpy as np
# import pandas as pd
#
# from track_analysis.components.md_common_python.py_common.logging import HoornLogger
# from track_analysis.components.track_analysis.legacy.audio_file_handler import AudioFileHandler
# from track_analysis.components.track_analysis.library.audio_transformation.feature_extraction.providers.calculated.rhythm.tempo_provider import \
#     BeatDetector
# from track_analysis.components.track_analysis.library.audio_transformation.key_extraction.feature.lof.lof_feature_transformer import (
#     LOFFeatureTransformer,
# )
# from track_analysis.components.track_analysis.library.audio_transformation.key_extraction.feature.vector.feature_vector_extractor import (
#     FeatureVectorExtractor,
# )
# from track_analysis.components.track_analysis.library.audio_transformation.key_extraction.preprocessing.note_extraction.note_extractor import (
#     NoteExtractor,
# )
# from track_analysis.components.track_analysis.library.audio_transformation.key_extraction.profile_generation.key_profile_merger import (
#     KeyProfileMerger,
# )
# from track_analysis.components.track_analysis.library.audio_transformation.key_extraction.profile_generation.key_profile_transposer import (
#     KeyProfileTransposer,
# )
# from track_analysis.components.track_analysis.library.audio_transformation.key_extraction.profile_generation.model.key_profile import (
#     KeyProfile,
# )
# from track_analysis.components.track_analysis.library.audio_transformation.key_extraction.profile_generation.single_track_profiler import (
#     SingleTrackProfiler,
# )
#
#
# class ProfileGenerator:
#     def __init__(self, logger: HoornLogger, audio_loader: AudioFileHandler, template_profile_normalized_to: int = 100, num_workers: int = 8):
#         self._logger = logger
#         self._separator = self.__class__.__name__
#
#         # build all the components exactly once
#         self._audio_loader: AudioFileHandler = audio_loader
#         self._beat_detector: BeatDetector = BeatDetector(logger)
#         self._note_extractor: NoteExtractor = NoteExtractor(logger)
#         self._feature_extractor: FeatureVectorExtractor = FeatureVectorExtractor(
#             logger, LOFFeatureTransformer()
#         )
#         self._track_profiler = SingleTrackProfiler(
#             audio_loader=self._audio_loader,
#             beat_detector=self._beat_detector,
#             note_extractor=self._note_extractor,
#             feature_extractor=self._feature_extractor,
#         )
#         self._key_profile_merger: KeyProfileMerger = KeyProfileMerger(logger)
#         self._key_profile_transposer: KeyProfileTransposer = KeyProfileTransposer(logger)
#         self._num_workers: int = num_workers
#
#         self._processed: int = 0
#         self._total: int = 0
#
#         self._normalized_to = template_profile_normalized_to
#         self._logger.trace("Successfully initialized.", separator=self._separator)
#
#     def generate_profile(self, corpus_file_path: Path) -> None:
#         self._logger.debug(
#             f"Generating profiles using corpus: \"{corpus_file_path}\"",
#             separator=self._separator,
#         )
#
#         corpus = self._read_corpus(corpus_file_path)
#         key_profiles = self._profile_all_tracks(corpus)
#
#         self._logger.info(
#             f"Successfully generated {len(key_profiles)} profiles out of {len(corpus)}.",
#             separator=self._separator,
#         )
#
#         merged_profiles = self._key_profile_merger.merge(key_profiles)
#         shifted_profiles = self._key_profile_transposer.transpose_profiles_to_c(merged_profiles)
#         self._print_profiles(shifted_profiles)
#
#     @staticmethod
#     def _read_corpus(path: Path) -> pd.DataFrame:
#         """
#         Loads exactly the two required columns: 'track_path' and 'track_key'.
#         Raises if missing.
#         """
#         df = pd.read_csv(
#             path,
#             usecols=["Path", "Principal Key"],
#             header=0
#         )
#         df = df.rename(columns={"Path": "track_path", "Principal Key": "track_key"})
#         return df
#
#     def _profile_all_tracks(self, corpus: pd.DataFrame) -> List[KeyProfile]:
#         """
#         Uses a ThreadPoolExecutor to profile every track in 'corpus'. Returns
#         a list of successfully built KeyProfiles.
#         """
#         total = len(corpus)
#
#         self._processed = 0
#         self._total = total
#
#         with ThreadPoolExecutor(max_workers=self._num_workers) as executor:
#             future_to_meta = self._submit_track_jobs(executor, corpus)
#             key_profiles = self._collect_track_results(future_to_meta)
#
#         return key_profiles
#
#     def _submit_track_jobs(
#             self,
#             executor: ThreadPoolExecutor,
#             corpus: pd.DataFrame
#     ) -> dict:
#         """
#         Submits each (track_path, track_key) to the thread pool.
#         Returns a mapping: Future -> (raw_path, track_key, idx).
#         """
#         future_to_meta = {}
#         for idx, row in enumerate(corpus.itertuples(index=False), start=1):
#             raw_path = Path(row.track_path)  # type: ignore
#             track_key = row.track_key  # type: ignore
#
#             if not raw_path.exists():
#                 self._logger.warning(
#                     f"Track \"{track_key}\" not found on disk (skipping).",
#                     separator=self._separator,
#                 )
#                 continue
#
#             pct = idx / self._total * 100
#             self._logger.debug(
#                 f"Scheduling track ({idx}/{self._total} [{pct:.4f}%]): \"{raw_path}\"",
#                 separator=self._separator,
#             )
#
#             future = executor.submit(self._track_profiler.profile, raw_path, track_key)
#             future_to_meta[future] = (raw_path, track_key)
#
#         return future_to_meta
#
#     def _collect_track_results(
#             self,
#             future_to_meta: dict
#     ) -> List[KeyProfile]:
#         """
#         Waits for each Future in future_to_meta to complete. On success,
#         collects the KeyProfile; on failure, logs an error.
#         """
#         collected: List[KeyProfile] = []
#         for future in as_completed(future_to_meta):
#             self._processed += 1
#             raw_path, track_key = future_to_meta[future]
#             try:
#                 kp: KeyProfile = future.result()
#                 collected.append(kp)
#                 pct = self._processed / self._total * 100
#                 self._logger.debug(
#                     f"Completed track ({self._processed}/{self._total} [{pct:.4f}%]): "
#                     f"\"{raw_path}\" â†’ {kp.get_label()}",
#                     separator=self._separator,
#                 )
#             except Exception as exc:
#                 self._logger.error(
#                     f"Error profiling \"{raw_path}\" (label={track_key}): {exc}",
#                     separator=self._separator,
#                 )
#         return collected
#
#     def _print_profiles(self, shifted_profiles: List[KeyProfile]) -> None:
#         for profile in shifted_profiles:
#             mode_vector = self._get_vector_for_mode(profile)
#             label = profile.get_label()
#
#             self._logger.debug(f"Generated vector for {label} based on {len(profile.vectors)} tracks.", separator=self._separator)
#
#             self._logger.info(
#                 f"Template Profile (normalized/precise) for {label}:\n"
#                 f"{self._format_vector(mode_vector, decimals=18)}",
#                 separator=self._separator,
#             )
#             self._logger.info(
#                 f"Template Profile (normalized/rounded) for {label}:\n"
#                 f"{self._format_vector(mode_vector, decimals=2)}",
#                 separator=self._separator,
#             )
#
#             self._logger.info("-------------------------------------------------------------", separator=self._separator)
#
#     def _get_vector_for_mode(self, profile: KeyProfile) -> np.ndarray:
#         median_profile: np.ndarray = profile.geometric_median()
#         self._logger.info(
#             f"Template Profile (raw) for {profile.get_label()}:\n{pprint.pformat(median_profile)}",
#             separator=self._separator,
#         )
#
#         median_profile_sum = median_profile.sum()
#         if median_profile_sum == 0:
#             self._logger.warning(
#                 f"Median profile for {profile.get_label()} summed to zero; returning zeros.",
#                 separator=self._separator,
#             )
#             return np.zeros_like(median_profile)
#
#         scaling_factor = self._normalized_to / median_profile_sum
#         scaled_profile = median_profile * scaling_factor
#         return scaled_profile
#
#     @staticmethod
#     def _format_vector(vec: np.ndarray, decimals: int = 4) -> str:
#         fmt = f"{{:.{decimals}f}}"
#         return "[" + ", ".join(fmt.format(x) for x in vec) + "]"

# TODO - Recreate this later.
